#ceph介绍：
ceph统一的、分布式的存储系统，可以提供对象存储、块存储和文件系统存储三种功能。
Based upon RADOS, Ceph Storage Clusters consist of two types of daemons: a Ceph OSD Daemon (OSD) stores data as objects on a storage node; and a Ceph Monitor (MON) maintains a master copy of the cluster map. A Ceph Storage Cluster may contain thousands of storage nodes. A minimal system will have at least one Ceph Monitor and two Ceph OSD Daemons for data replication.
The Ceph Filesystem, Ceph Object Storage and Ceph Block Devices read data from and write data to the Ceph Storage Cluster.
ceph Filesystem clients需要ceph Metadata Server，ceph Block Devices和Ceph Object Storage不用MDS。
---
http://www.ibm.com/developerworks/cn/cloud/library/cl-openstackceph/
与任何经典的分布式文件系统中一样，放入集群中的文件是条带化的，依据一种称为Ceph Controlled Replication Under
Scalable Hashing(CRUSH)的伪随机的数据分布算法放入集群节点中。

---
根据Ceph官方文档推荐，一个ceph storage cluster需要至少一个ceph monitor和至少两个ceph OSD Daemons来获得avtive-clean的状态.
step1: 利用docker在集群中布置一个ceph monitor和多个ceph OSD Daemons
202作monitor，203作OSD，
参考：Running Ceph inside Docker
在docker hub里有很多的镜像，我们使用Ceph的命名空间，镜像前缀都是ceph/<daemon>
1.1 启动MON
docker run -d --net=host -v /etc/ceph:/etc/ceph -v /var/lib/ceph/:/var/lib/ceph -e MON_IP=10.10.10.202 -e CEPH_PUBLIC_NETWORK=10.10.10.202/24 ceph/daemon mon
然后查看状态
docker ps
docker exec dockerID ceph -s
查看生成的ceph.conf和ceph keyring文件
ls /etc/ceph/
ls /var/lib/ceph/
ls /var/lib/ceph/mon/ceph-zodiac-02
![](http://docs.ceph.com/docs/master/_images/ditaa-cffd08dd3e192a5f1d724ad7930cb04200b9b425.png)
1.2 启动OSD：
docker run -d --net=host --privileged=true -v /etc/ceph:/etc/ceph -v /var/lib/ceph/:/var/lib/ceph -v /dev/:/dev/ -e OSD_DEVICE=/dev/vdd ceph-daemon osd_ceph_disk
Q：这样执行不通，改为下面
docker run -d --net=host -v /etc/ceph:/etc/ceph -v /var/lib/ceph:/var/lib/ceph \
-v /dev:/dev --privileged=true -e OSD_FORCE_ZAP=1 \
-e OSD_DEVICE=/dev/sdb ceph/daemon osd_ceph_disk
启动osd进程
docker exec 7c2ceeb668d3 ceph-osd -i 0/1
查看osd状态
docker exec 7c2ceeb668d3 ceph osd stat


docker run -d --net=host \
--privileged=true \
--pid=host \
-e JOURNAL_DIR=/var/lib/ceph/osd/journal \
-e HOSTNAME=10.10.10.205 \
-e KV_TYPE=etcd \
-e KV_IP=127.0.0.1 \
-e OSD_DEVICE=/dev/sdb \
-e OSD_FORCE_ZAP=1 \
-v /var/log/ceph:/var/log/ceph \
-v /var/lib/ceph:/var/lib/ceph \
-v /dev/:/dev/ \
ceph/daemon osd
---提示Waiting for monitor setup to complete.

ExecStart=/usr/bin/docker run --rm --name %p --net=host \
-v /var/lib/ceph:/var/lib/ceph \
-e MON_IP=${COREOS_PUBLIC_IPV4}:6789 \
-e CEPH_PUBLIC_NETWORK=10.21.147.32/27 \
-e CEPH_PRIVATE_NETWORK=172.16.42.0/24 \
-e MON_NAME=%H \
-e KV_TYPE=etcd \
-e KV_IP=127.0.0.1 \
ceph/daemon mon
MON_IP和CEPH_PUBLIC_NETWORK的关系。

1.3 启动MDS:
docker run -d --net=host -v /etc/ceph:/etc/ceph -v /var/lib/ceph:/var/lib/ceph \
-e CEPHFS_CREATE=1 ceph/daemon mds
查看mds状态
ceph mds stat 
---------
在203布置OSD等模块：
scp -r core@10.10.10.202:/etc/ceph/ core@10.10.10.204:/etc/
集群间考文件时keyring：permission denied。

step2：ceph.conf和Ceph keyring都在step1中启动监测模块时生成，部署多节点，需要把config和keys分发到其他节点。
其他node如何获取ceph.con和ceph keyring等配置文件？上传方式有
1. scp：依赖公私钥，
2. ftp：coreos没有这个客户端，
3. http服务：接受其他的文件，也提供下载。
为了方便，在docker中部署，用etcd存储ceph.conf和ceph keyring等配置文件。
ceph：配置文件base64，通常比较，zip，base64—1MB，写到etcd后。
指定mon IP，mon.sh，zip打包，base64 => 每个service中wget脚本，然后在cloud-config中配置。


192中的上传目录：/usr/share/nginx/coreos-mirror

ceph.sh
sudo zip -r conf.zip /etc/ceph #压缩不要带路径

各服务器从192上下载，
wget http://10.10.10.192/conf.zip
sudo mv conf.zip /etc/ceph/
sudo unzip conf.zip
echo "$PWD"
etcd中解压

各自配置conf文件
- name: "install-k8s.service"
command: restart
content: |
[Unit]
Description=Install k8s services
[Service]
{{- with .K8sRole }}
ExecStart=cd /home/core && /usr/bin/wget http://10.10.10.192/install-k8s/{{ . }}.zip && unzip {{ . }}.zip && cd {{ . }} && bash setup_k8s_{{ . }}.sh
{{- else }}
ExecStart=cd /home/core && /usr/bin/wget http://10.10.10.192/install-k8s/worker.zip && unzip worker.zip && cd worker && bash setup_k8s_worker.sh
{{- end }}
RemainAfterExit=yes
Type=oneshot

ExecStart=cd /etc/ceph && /usr/bin/wget http://10.10.10.192/conf.zip && sudo unzip conf.zip && bash ceph.sh




首先在单机中测试ceph模块service
https://github.com/ceph/ceph-docker/tree/master/examples/coreos/units-etcd

在cloud-config中写入ceph的service的配置文档。


add mon,mds
PS：每台主机只能为ceph集群启动一个mon进程
PS2：集群中只有两个mon节点时，其中一个断开会导致集群不能访问，建议mon总数3个或以上。


youtube视频中的步骤
getenforce
./1-selinux
./2-mon
运行mon
sudo docker run -d --net=host -v /etc/ceph:/etc/ceph -v /var/lib/ceph/:/var/lib/ceph/ - e \
MON_IP=192.168.0.69 -e CEPH_PUBLIC_NETWORK=192.168.0.0/24 ceph/daemon mon
sudo docker ps
sudo docker exec CONTAINER ID ceph -s
#now i need to distribute those config and keys to the 2 other node
拷贝/etc/ceph/下四个配置文件和/var/lib/ceph/bootstrap-rgw|bootstrap-mds|bootstrap-osd/ceph.keyring（三个）到另外两个node。
接下来切换到另外两个node，应用这些config
./3-copy
#now I'm going to move the 2 others and apply the config
ls /etc/ceph
#now I'm going to run another monitor
容器3：
sudo docker run -d --net=host -v /etc/ceph:/etc/ceph -v /var/lib/ceph/:/var/lib/ceph/ -e \
MON_IP=192.168.0.71 -e CEPH_PUBLIC_NETWORK=192.168.0.0/24 ceph/daemon mon
容器2：
sudo docker run -d --net=host -v /etc/ceph:/etc/ceph -v /var/lib/ceph/:/var/lib/ceph/ -e \
MON_IP=192.168.0.70 -e CEPH_PUBLIC_NETWORK=192.168.0.0/24 ceph/daemon mon

向总理请教：
现在一共有三个安装脚本，分别是install-first-mon.sh,install-osd.sh,install-other-mon.sh
只要在cloud-config里写不同的服务，服务里wget相应的脚本并执行就可以了。类似文山
ExecStart=wget http://...sh&&bash yyy.sh
全自动安装流程：配置通过etcd写入，脚本通过http下载
1.mon在某台机器上执行，写入etcd。其他的mon获取etcd，然后运行自己的mon。
2.其他机器在cloud-config中，执行服务，获取,运行三个脚本。

scp core@10.10.10.206:~/install*.sh ./ 再加上一个install-other-mon.sh文件，一共4个，拷贝到192上。实际上不需要install-first-mon.sh
scp install-*.sh atlas@10.10.10.192:~/install-ceph/
在192上移动/install-ceph/到/usr/share/nginx/http/install-ceph
sudo cp ~/install-ceph/* /usr/share/nginx/http/install-ceph/

ExecStart=/usr/bin/wget http://10.10.10.192/conf.zip && sudo unzip conf.zip && bash install-other-mon.sh
wget难道只能下载一个文件。在客户端将sh压缩，
zip ceph-install.zip ./install*.sh
scp ceph-install.zip atlas@10.10.10.192:~/install-ceph/
sudo cp ~/install-ceph/ceph-install.zip /usr/share/nginx/http/install-ceph/
/usr/bin/wget http://10.10.10.192/install-ceph/conf.zip

cloud-config中的service配置
ExecStart=cd /etc/ceph && /usr/bin/wget http://10.10.10.192/install-ceph/ceph-install.zip && sudo unzip conf.zip && bash install-other-mon.sh
&& bash install-osd.sh && bash install-mds.sh



除了这三个，我在205上运行了mds.sh脚本。
后续的问题：
7个文件，一个是binary，全部都用base64转一遍写进去。
etcd的value长度有没有限制，https://github.com/coreos/etcd/issues/2992, https://github.com/coreos/etcd/issues/1548
如何实现全自动
集群中不同机器之间mon的关系。
如何利用三种不同的存储方式。






















